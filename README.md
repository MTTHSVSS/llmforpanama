# Analyzing the Paradise Papers

## Overview

This project is focusing on me tinkering with textual and numerical data. I've chosen the Paradise Papers Dataset, because I am fascinated by economic crime and I knew it would keep me interested. 

The Paradise Papers is a vast collection of data revealing offshore investments by a wide array of individuals and corporations, shedding light on numerous tax jurisdictions.

## What I've done:

1. **Data Collection**: Identified and collected the Paradise Papers dataset from Kaggle.

2. **Data Preprocessing**: Cleaned, normalized, and tokenized the dataset. This includes the removal of stopwords, punctuation, and the conversion of text to lower case.

3. **Data Exploration**: Conducted a thorough analysis of the dataset's structure and content to gain crucial insights aka. I've created some graphs.

4. **Feature Extraction**: Converted the cleaned data into a numerical form through CountVectorizer.

## What I want to do:

1. **Named Entity Recognition**: Identify important entities (like names of people, companies, countries, etc.) in the text.

2. **Link Prediction**: Predict whether a link (connection) should exist between two nodes (entities) based on the other links in the network.

3. **Community Detection**: Use Community Detection algorithms to identify clusters or communities within the network of entities.

4. **Fine-Tuned LLM** Fine-tune a large language model (LLM) that answers my questions about the Paradise Panama Papers.